{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM From Scratch\n",
    "\n",
    "1. LSTM Unit\n",
    "2. LSTM Layer\n",
    "3. LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# define tanh function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a single LSTM Unit\n",
    "def LSTMUnit(x, h, c, W_hh, W_ih, b):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    x: np array of shape (batch_size, input_size)\n",
    "    h: np array of shape (batch_size, hidden_size)\n",
    "    c: np array of shape (batch_size, hidden_size)\n",
    "    W_hh: np array of shape (4 * hidden_size, hidden_size)\n",
    "    W_ih: np array of shape (4 * hidden_size, input_size)\n",
    "    b: np array of shape (4 * hidden_size)\n",
    "    \"\"\"\n",
    "    # generate the inputs to forget gate, input gate, candidate gate, and output gate\n",
    "    i, f, g, o = np.split(W_hh@h + W_ih@x + b, 4)\n",
    "    i, f, g, o = sigmoid(i), sigmoid(f), tanh(g), sigmoid(o)\n",
    "    c_out = f*c + i*g\n",
    "    h_out = o * tanh(c_out)\n",
    "    return h_out, c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "batch_size = 1\n",
    "input_size = 20\n",
    "hidden_size = 100\n",
    "model = nn.LSTMCell(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that LSTMUnit is working correctly\n",
    "x = np.random.randn(1, 20).astype(np.float32)\n",
    "h0 = np.random.randn(1, 100).astype(np.float32)\n",
    "c0 = np.random.randn(1, 100).astype(np.float32)\n",
    "\n",
    "h_, c_ = model(torch.tensor(x)), (torch.tensor(h0), torch.tensor(c0))\n",
    "\n",
    "h, c = LSTMUnit(x[0], h0[0], c0[0],\n",
    "                model.weight_hh.detach().numpy(),\n",
    "                model.weight_ih.detach().numpy(),\n",
    "                (model.bias_hh + model.bias_ih).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4048018"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(h_[0].detach().numpy() - h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.LSTM(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "batch_size = 50 # 50 sequeces\n",
    "input_size = 20\n",
    "hidden_size = 100\n",
    "X = np.random.randn(batch_size, input_size).astype(np.float32)\n",
    "h0 = np.random.randn(1, hidden_size).astype(np.float32)\n",
    "c0 = np.random.randn(1, hidden_size).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMLayer(X, h, c, W_hh, W_ih, b):\n",
    "    H = np.zeros((X.shape[0], h.shape[0]))\n",
    "    for t in range(X.shape[0]):\n",
    "        h, c = LSTMUnit(X[t], h, c, W_hh, W_ih, b)\n",
    "        H[t] = h\n",
    "    return H, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, cn = LSTMLayer(X, h0[0], c0[0],\n",
    "                model.weight_hh_l0.detach().numpy(),\n",
    "                model.weight_ih_l0.detach().numpy(),\n",
    "                (model.bias_hh_l0 + model.bias_ih_l0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_, (hn, cn) = model(torch.tensor(X)[:, None, :],\n",
    "                     (torch.tensor(h0)[:, None, :],\n",
    "                      torch.tensor(c0)[:, None, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4628261130620635e-06"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(H - H_[:, 0, :].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a single LSTM Unit\n",
    "def LSTMUnit(x, h, c, W_hh, W_ih, b):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    x: np array of shape (batch_size, input_size)\n",
    "    h: np array of shape (batch_size, hidden_size)\n",
    "    c: np array of shape (batch_size, hidden_size)\n",
    "    W_hh: np array of shape (4 * hidden_size, hidden_size)\n",
    "    W_ih: np array of shape (4 * hidden_size, input_size)\n",
    "    b: np array of shape (4 * hidden_size)\n",
    "    \"\"\"\n",
    "    # generate the inputs to forget gate, input gate, candidate gate, and output gate\n",
    "    i, f, g, o = np.split(h @ W_hh + x @ W_ih + b, 4, axis=1) # now first dim is batch dim, second dim is hidden dim\n",
    "    i, f, g, o = sigmoid(i), sigmoid(f), tanh(g), sigmoid(o)\n",
    "    c_out = f*c + i*g\n",
    "    h_out = o * tanh(c_out)\n",
    "    return h_out, c_out\n",
    "\n",
    "def LSTMLayer(X, h, c, W_hh, W_ih, b):\n",
    "    H = np.zeros((X.shape[0], X.shape[1], h.shape[1]))\n",
    "    for t in range(X.shape[0]):\n",
    "        h, c = LSTMUnit(X[t], h, c, W_hh, W_ih, b)\n",
    "        H[t] = h\n",
    "    return H, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "batch_size = 128\n",
    "sequence_length = 50\n",
    "input_size = 20\n",
    "hidden_size = 100\n",
    "X = np.random.randn(sequence_length, batch_size, input_size).astype(np.float32)\n",
    "h0 = np.random.randn(1, batch_size, hidden_size).astype(np.float32)\n",
    "c0 = np.random.randn(1, batch_size, hidden_size).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_, (hn, cn) = model(torch.tensor(X),\n",
    "                     (torch.tensor(h0),\n",
    "                      torch.tensor(c0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 128, 100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, hn_cn = LSTMLayer(X, h0[0], c0[0],\n",
    "                model.weight_hh_l0.detach().numpy().T,\n",
    "                model.weight_ih_l0.detach().numpy().T,\n",
    "                (model.bias_hh_l0 + model.bias_ih_l0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.940139810370896e-06"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(H-H_.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 128, 100)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 100)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn, cn = hn_cn\n",
    "cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a linear activation function\n",
    "# takes in as input a tensor of shape (batch_size, hidden_size)\n",
    "# returns a tensor of shape (batch_size, output_size)\n",
    "def Linear(x, W, b):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    x: np array of shape (batch_size, hidden_size) \n",
    "    W: np array of shape (output_size, hidden_size)\n",
    "    b: np array of shape (output_size)\n",
    "    Output\n",
    "    y: np array of shape (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    return x @ W.T + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,32,400) (1,128,100) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(output_size, hidden_size)\n\u001b[1;32m     23\u001b[0m b_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(output_size)\n\u001b[0;32m---> 25\u001b[0m y_, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mW_hh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 3\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(X, h, c, W_hh, W_ih, b, W, b_)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(X, h, c, W_hh, W_ih, b, W, b_):\n\u001b[0;32m----> 3\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_hh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     y \u001b[38;5;241m=\u001b[39m Linear(h, W, b_)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y, h, c\n",
      "Cell \u001b[0;32mIn[52], line 22\u001b[0m, in \u001b[0;36mLSTMLayer\u001b[0;34m(X, h, c, W_hh, W_ih, b)\u001b[0m\n\u001b[1;32m     20\u001b[0m H \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 22\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[43mLSTMUnit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_hh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     H[t] \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m H, (h, c)\n",
      "Cell \u001b[0;32mIn[52], line 15\u001b[0m, in \u001b[0;36mLSTMUnit\u001b[0;34m(x, h, c, W_hh, W_ih, b)\u001b[0m\n\u001b[1;32m     13\u001b[0m i, f, g, o \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(h \u001b[38;5;241m@\u001b[39m W_hh \u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m@\u001b[39m W_ih \u001b[38;5;241m+\u001b[39m b, \u001b[38;5;241m4\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# now first dim is batch dim, second dim is hidden dim\u001b[39;00m\n\u001b[1;32m     14\u001b[0m i, f, g, o \u001b[38;5;241m=\u001b[39m sigmoid(i), sigmoid(f), tanh(g), sigmoid(o)\n\u001b[0;32m---> 15\u001b[0m c_out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mc\u001b[49m \u001b[38;5;241m+\u001b[39m i\u001b[38;5;241m*\u001b[39mg\n\u001b[1;32m     16\u001b[0m h_out \u001b[38;5;241m=\u001b[39m o \u001b[38;5;241m*\u001b[39m tanh(c_out)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h_out, c_out\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,32,400) (1,128,100) "
     ]
    }
   ],
   "source": [
    "# define a forward pass through a single layer\n",
    "def forward(X, h, c, W_hh, W_ih, b, W, b_):\n",
    "    h, c = LSTMLayer(X, h, c, W_hh, W_ih, b)\n",
    "    y = Linear(h, W, b_)\n",
    "    return y, h, c\n",
    "\n",
    "# define the model\n",
    "batch_size = 128\n",
    "sequence_length = 50\n",
    "input_size = 20\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "X = np.random.randn(sequence_length, batch_size, input_size).astype(np.float32)\n",
    "h0 = np.random.randn(1, batch_size, hidden_size).astype(np.float32)\n",
    "c0 = np.random.randn(1, batch_size, hidden_size).astype(np.float32)\n",
    "\n",
    "# get weight from pytorch model\n",
    "model = nn.LSTM(input_size, hidden_size)\n",
    "W_hh = model.weight_hh_l0.detach().numpy().T\n",
    "W_ih = model.weight_ih_l0.detach().numpy().T\n",
    "b = (model.bias_hh_l0 + model.bias_ih_l0).detach().numpy()\n",
    "W = np.random.randn(output_size, hidden_size)\n",
    "b_ = np.random.randn(output_size)\n",
    "\n",
    "y_, (hn, cn) = forward(X, h0, c0,\n",
    "                       W_hh, W_ih, b, W, b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lstmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
